activation_fuctions = {
    'sigmoid' : 'Sigmoid',
    'tanh': 'Tanh',
    'softplus': 'Softplus',
    'silu': 'SiLU',
    'relu': 'ReLU',
}

loss_fuctions = {
    'mse': 'MSELoss',
    'bce': 'BCELoss'
}

optimizers = {
    'adam': 'Adam',
    'sgd': 'SGD',
    'rmsprop': 'RMSprop',
    'adadelta': 'Adadelta',
    'adamax': 'Adamax',
    'adagrad': 'Adagrad',
    'nadam': 'NAdam'
}
